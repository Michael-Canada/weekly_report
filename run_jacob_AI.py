import subprocess
import pandas as pd
import os
from datetime import datetime
import sys
from concurrent.futures import ThreadPoolExecutor
import time

'''
Environment: placebo_api_local
'''

CURRENT_ISO = "miso"

def run_script(script_name):
    """Run a Python script and return the result"""
    try:
        print(f"Starting {script_name}...")
        result = subprocess.run([sys.executable, script_name], 
                              capture_output=True, 
                              text=True, 
                              cwd=os.getcwd())
        
        if result.returncode == 0:
            print(f"✓ {script_name} completed successfully")
            return True, result.stdout
        else:
            print(f"✗ {script_name} failed with error:")
            print(result.stderr)
            return False, result.stderr
            
    except Exception as e:
        print(f"✗ Error running {script_name}: {str(e)}")
        return False, str(e)

def merge_csv_files():
    """Merge the two CSV files generated by the scripts"""
    try:
        # Get today's date
        today_date = datetime.today().strftime("%Y-%m-%d")
        
        # Define file paths
        # confusion_matrix_file = f"confusion_matrix_per_constraint_MISO_{today_date}.csv"
        # forecast_performance_file = f"Forecast_performace_evaluation_miso_{today_date}.csv"
        confusion_matrix_file = f"confusion_matrix_per_constraint_{CURRENT_ISO.upper()}_{today_date}.csv"
        forecast_performance_file = f"Forecast_performace_evaluation_{CURRENT_ISO.lower()}_{today_date}.csv"
        
        # Check if both files exist
        if not os.path.exists(confusion_matrix_file):
            print(f"✗ File not found: {confusion_matrix_file}")
            return False
            
        if not os.path.exists(forecast_performance_file):
            print(f"✗ File not found: {forecast_performance_file}")
            return False
        
        print(f"Loading {confusion_matrix_file}...")
        # Load confusion matrix file (MultiIndex on monitored_uid and contingency_uid)
        df_confusion = pd.read_csv(confusion_matrix_file, index_col=[0, 1])
        
        print(f"Loading {forecast_performance_file}...")
        # Load forecast performance file (MultiIndex on monitored_uid and contingency_uid)
        df_forecast = pd.read_csv(forecast_performance_file, index_col=[0, 1])
        
        # merged_df_hr.columns = merged_df_hr.columns.droplevel(2)

        print("Merging dataframes...")
        # Both dataframes now have the same MultiIndex structure (monitored_uid, contingency_uid)
        # Reset index to make columns available for processing
        df_forecast_reset = df_forecast.reset_index()
        df_confusion_reset = df_confusion.reset_index()

        # Remove unwanted columns from forecast data  
        f1_columns = [col for col in df_forecast_reset.columns if col in ["Shadow Price.3", "Shadow Price.1"]]
        if f1_columns:
            df_forecast_reset = df_forecast_reset.drop(columns=f1_columns)

        df_forecast_reset = df_forecast_reset.iloc[2:]  # Remove first two rows
        
        # Rename columns for better clarity
        column_rename_map = {
            "Median % Rating": "Median % Rating during binding events",
            "Median % Rating.1": "Median % Rating during Not Binding Time", 
            "Shadow Price": "Shadow Price num of hours",
            "Shadow Price.2": "Shadow Price total price",
            "Statistics": "Predictor",
            "Statistics.1": "Confidence"
        }
        df_forecast_reset = df_forecast_reset.rename(columns=column_rename_map)
        
        print("Debugging structures:")
        print(f"Confusion columns: {list(df_confusion_reset.columns)}")
        print(f"Forecast columns: {list(df_forecast_reset.columns)}")
        print(f"Confusion shape: {df_confusion_reset.shape}")
        print(f"Forecast shape: {df_forecast_reset.shape}")
        
        # Rename the index columns to match between dataframes
        df_confusion_reset = df_confusion_reset.rename(columns={'level_0': 'monitored_uid', 'level_1': 'contingency_uid'})
        df_forecast_reset = df_forecast_reset.rename(columns={'level_0': 'monitored_uid', 'level_1': 'contingency_uid'})
        
        # Merge on both index columns (monitored_uid, contingency_uid)
        merged_df = df_confusion_reset.merge(df_forecast_reset, 
                                           on=['monitored_uid', 'contingency_uid'], 
                                           how='outer')
        
        # Set the merged index back to the two columns
        merged_df = merged_df.set_index(['monitored_uid', 'contingency_uid'])
        
        merged_df.sort_values(by=["Predictor", "Confidence", "F1", "Shadow Price num of hours"], 
                             ascending=[False, False, True, False], 
                             inplace=True)
        
        # move the columns "Predictor" and "Confidence" to the front
        cols = merged_df.columns.tolist()
        for col in ["Shadow Price total price","Shadow Price num of hours", "F1", "Confidence", "Predictor"]:
            if col in cols:
                cols.insert(0, cols.pop(cols.index(col)))
        merged_df = merged_df[cols]
        
        # Save merged file
        output_file = f"merged_analysis_miso_{today_date}.csv"
        merged_df.to_csv(output_file)
        
        print(f"✓ Merged file saved as: {output_file}")
        print(f"✓ Merged dataframe shape: {merged_df.shape}")
        print(f"✓ Index names: {merged_df.index.names}")
        
        return merged_df
        
    except Exception as e:
        print(f"✗ Error merging CSV files: {str(e)}")
        return False

def Analyze_AI(merge_data):
    """Analyze the merged data and add prediction quality assessment"""
    if merge_data is False:
        print("No merged data to analyze.")
        return
    
    try:
        print("\nAnalyzing merged data...")
        print(f"Input data shape: {merge_data.shape}")
        print(f"Columns: {list(merge_data.columns)}")
        
        # Create a copy to avoid modifying the original
        analyzed_data = merge_data.copy()
        
        # Calculate quantiles for dynamic thresholds
        def safe_numeric_series(series):
            """Safely convert series to numeric, handling strings and NaN"""
            return pd.to_numeric(series, errors='coerce').dropna()
        
        print("Calculating dynamic thresholds based on data distribution...")
        
        # Calculate quantiles for each metric
        f1_values = safe_numeric_series(analyzed_data['F1'])
        confidence_values = safe_numeric_series(analyzed_data['Confidence'])
        precision_values = safe_numeric_series(analyzed_data['precision'])
        recall_values = safe_numeric_series(analyzed_data['recall'])
        pvalue_values = safe_numeric_series(analyzed_data['p-value'])
        predictor_values = safe_numeric_series(analyzed_data['Predictor'])
        
        # Calculate thresholds
        thresholds = {}
        
        # F1 thresholds
        thresholds['f1'] = {
            'excellent': max(f1_values.quantile(0.90), 0.5) if len(f1_values) > 0 else 0.5,
            'good': max(f1_values.quantile(0.75), 0.1) if len(f1_values) > 0 else 0.1,
            'fair': f1_values.quantile(0.50) if len(f1_values) > 0 else 0.0
        }
        
        # Confidence thresholds  
        thresholds['confidence'] = {
            'high': confidence_values.quantile(0.90) if len(confidence_values) > 0 else 0.5,
            'medium': confidence_values.quantile(0.75) if len(confidence_values) > 0 else 0.25,
            'low': confidence_values.quantile(0.50) if len(confidence_values) > 0 else 0.14
        }
        
        # Precision thresholds
        thresholds['precision'] = {
            'high': max(precision_values.quantile(0.90), 0.5) if len(precision_values) > 0 else 0.5,
            'medium': precision_values.quantile(0.75) if len(precision_values) > 0 else 0.1
        }
        
        # Recall thresholds
        thresholds['recall'] = {
            'high': max(recall_values.quantile(0.90), 0.5) if len(recall_values) > 0 else 0.5,
            'medium': recall_values.quantile(0.75) if len(recall_values) > 0 else 0.1
        }
        
        # P-value thresholds (statistical significance)
        thresholds['pvalue'] = {
            'highly_sig': 0.01,  # Standard statistical threshold
            'significant': 0.05,  # Standard statistical threshold
            'marginal': pvalue_values.quantile(0.75) if len(pvalue_values) > 0 else 0.27
        }
        
        # Predictor thresholds (reasonable range around central tendency)
        predictor_q25 = predictor_values.quantile(0.25) if len(predictor_values) > 0 else 0.6
        predictor_q75 = predictor_values.quantile(0.75) if len(predictor_values) > 0 else 0.9
        predictor_median = predictor_values.quantile(0.50) if len(predictor_values) > 0 else 0.8
        
        thresholds['predictor'] = {
            'optimal_min': max(predictor_median - 0.15, predictor_q25),
            'optimal_max': min(predictor_median + 0.15, predictor_q75),
            'reasonable_min': predictor_q25 - 0.2,
            'reasonable_max': predictor_q75 + 0.2
        }
        
        # Print calculated thresholds for transparency
        print("Calculated thresholds:")
        for metric, thresh in thresholds.items():
            print(f"  {metric}: {thresh}")
        
        def evaluate_prediction_quality(row):
            """
            Evaluate prediction quality based on multiple criteria using dynamic thresholds:
            - High F1 score (balance of precision and recall)
            - High confidence 
            - Reasonable predictor value (not too extreme)
            - Good precision and recall individually
            - Statistical significance (p-value)
            """
            
            def safe_numeric(value, default=0):
                """Safely convert to numeric, handling strings and NaN"""
                try:
                    if pd.isna(value):
                        return default
                    return float(value)
                except (ValueError, TypeError):
                    return default
            
            # Initialize score components
            scores = {}
            
            # 1. F1 Score evaluation - using calculated quantiles
            f1 = safe_numeric(row['F1'], 0)
            if f1 >= thresholds['f1']['excellent']:
                scores['f1'] = 'Excellent'
            elif f1 >= thresholds['f1']['good']:
                scores['f1'] = 'Good' 
            elif f1 >= thresholds['f1']['fair']:
                scores['f1'] = 'Fair'
            else:
                scores['f1'] = 'Poor'
            
            # 2. Confidence evaluation - using calculated quantiles
            confidence = safe_numeric(row['Confidence'], 0)
            if confidence >= thresholds['confidence']['high']:
                scores['confidence'] = 'High'
            elif confidence >= thresholds['confidence']['medium']:
                scores['confidence'] = 'Medium'
            elif confidence >= thresholds['confidence']['low']:
                scores['confidence'] = 'Low'
            else:
                scores['confidence'] = 'Very Low'
            
            # 3. Precision evaluation - using calculated quantiles
            precision = safe_numeric(row['precision'], 0)
            if precision >= thresholds['precision']['high']:
                scores['precision'] = 'High'
            elif precision >= thresholds['precision']['medium']:
                scores['precision'] = 'Medium'
            else:
                scores['precision'] = 'Low'
            
            # 4. Recall evaluation - using calculated quantiles
            recall = safe_numeric(row['recall'], 0)
            if recall >= thresholds['recall']['high']:
                scores['recall'] = 'High'
            elif recall >= thresholds['recall']['medium']:
                scores['recall'] = 'Medium'
            else:
                scores['recall'] = 'Low'
            
            # 5. Statistical significance - using standard thresholds and calculated marginal threshold
            p_value = safe_numeric(row['p-value'], 1.0)
            if p_value <= thresholds['pvalue']['highly_sig']:
                scores['significance'] = 'Highly Significant'
            elif p_value <= thresholds['pvalue']['significant']:
                scores['significance'] = 'Significant'
            elif p_value <= thresholds['pvalue']['marginal']:
                scores['significance'] = 'Marginally Significant'
            else:
                scores['significance'] = 'Not Significant'
            
            # 6. Predictor value reasonableness - using calculated quantile-based ranges
            predictor = safe_numeric(row['Predictor'], 0)
            if thresholds['predictor']['optimal_min'] <= predictor <= thresholds['predictor']['optimal_max']:
                scores['predictor'] = 'Optimal'
            elif thresholds['predictor']['reasonable_min'] <= predictor <= thresholds['predictor']['reasonable_max']:
                scores['predictor'] = 'Reasonable'
            elif predictor < thresholds['predictor']['reasonable_min']:
                scores['predictor'] = 'Extreme Low'
            else:
                scores['predictor'] = 'Extreme High'
                
            
            # 7. Overall assessment based on combination of factors
            overall_score = 0
            
            # Weight the most important factors
            if scores['f1'] in ['Excellent', 'Good']:
                overall_score += 3
            elif scores['f1'] == 'Fair':
                overall_score += 1
            
            if scores['confidence'] in ['High', 'Medium']:
                overall_score += 2
            
            if scores['precision'] in ['High', 'Medium']:
                overall_score += 2
                
            if scores['recall'] in ['High', 'Medium']:
                overall_score += 2
            
            if scores['significance'] in ['Highly Significant', 'Significant']:
                overall_score += 1
            
            if scores['predictor'] in ['Optimal', 'Reasonable']:
                overall_score += 1
            
            # Determine overall quality
            if overall_score >= 9:
                overall = 'Excellent'
            elif overall_score >= 6:
                overall = 'Good'
            elif overall_score >= 3:
                overall = 'Fair'
            else:
                overall = 'Poor'
            
            return overall, scores
        
        # Apply the evaluation to each row
        print("Evaluating prediction quality for each constraint...")
        
        evaluation_results = []
        detailed_scores = []
        
        for idx, row in analyzed_data.iterrows():
            overall, scores = evaluate_prediction_quality(row)
            evaluation_results.append(overall)
            detailed_scores.append(scores)
        
        # Add the results as new columns (base categorical assessments)
        analyzed_data['Prediction_Quality'] = evaluation_results
        analyzed_data['F1_Category'] = [scores['f1'] for scores in detailed_scores]
        analyzed_data['Confidence_Level'] = [scores['confidence'] for scores in detailed_scores]
        analyzed_data['Precision_Level'] = [scores['precision'] for scores in detailed_scores]
        analyzed_data['Recall_Level'] = [scores['recall'] for scores in detailed_scores]
        analyzed_data['Statistical_Significance'] = [scores['significance'] for scores in detailed_scores]
        analyzed_data['Predictor_Reasonableness'] = [scores['predictor'] for scores in detailed_scores]

        # ------------------------------------------------------------------
        # Enhanced multi-dimensional analysis
        # ------------------------------------------------------------------
        def sn(val, default=0.0):
            """Safe numeric conversion for scalar values."""
            try:
                if pd.isna(val):
                    return default
                return float(val)
            except (TypeError, ValueError):
                return default

        # Pre-compute helper numeric series
        tp_series = pd.to_numeric(analyzed_data['TP'], errors='coerce').fillna(0)
        fp_series = pd.to_numeric(analyzed_data['FP'], errors='coerce').fillna(0)
        fn_series = pd.to_numeric(analyzed_data['FN'], errors='coerce').fillna(0)
        hours_series = pd.to_numeric(analyzed_data['Shadow Price num of hours'], errors='coerce').fillna(0)
        price_series = pd.to_numeric(analyzed_data['Shadow Price total price'], errors='coerce').fillna(0)
        median_binding_series = pd.to_numeric(analyzed_data['Median % Rating during binding events'], errors='coerce').fillna(0)
        median_not_series = pd.to_numeric(analyzed_data['Median % Rating during Not Binding Time'], errors='coerce').fillna(0)

        max_hours = max(hours_series.max(), 1)
        max_price = max(price_series.max(), 1)
        total_obs_series = tp_series + fp_series + fn_series
        max_obs = max(total_obs_series.max(), 1)

        # Flow prediction quality classification
        def classify_flow_pred(row):
            rating_gap = sn(row['Median % Rating during binding events']) - sn(row['Median % Rating during Not Binding Time'])
            conf = sn(row['Confidence'])
            p_val = sn(row['p-value'], 1.0)
            predictor = sn(row['Predictor'])

            gap_quality = 'Weak'
            if rating_gap >= 0.20:
                gap_quality = 'Strong'
            elif rating_gap >= 0.10:
                gap_quality = 'Moderate'
            elif rating_gap >= 0.05:
                gap_quality = 'Suggestive'

            # Significance bucket
            if p_val <= thresholds['pvalue']['highly_sig']:
                sig_bucket = 'Highly Significant'
            elif p_val <= thresholds['pvalue']['significant']:
                sig_bucket = 'Significant'
            elif p_val <= thresholds['pvalue']['marginal']:
                sig_bucket = 'Marginal'
            else:
                sig_bucket = 'Not Significant'

            # Confidence bucket pre-computed categories
            conf_bucket = 'Very Low'
            if conf >= thresholds['confidence']['high']:
                conf_bucket = 'High'
            elif conf >= thresholds['confidence']['medium']:
                conf_bucket = 'Medium'
            elif conf >= thresholds['confidence']['low']:
                conf_bucket = 'Low'

            # Predictor reasonableness already categorized
            predictor_bucket = 'Extreme'
            if thresholds['predictor']['optimal_min'] <= predictor <= thresholds['predictor']['optimal_max']:
                predictor_bucket = 'Optimal'
            elif thresholds['predictor']['reasonable_min'] <= predictor <= thresholds['predictor']['reasonable_max']:
                predictor_bucket = 'Reasonable'

            # Combine into overall flow prediction quality
            score = 0
            score += {'Strong': 3, 'Moderate': 2, 'Suggestive': 1, 'Weak': 0}[gap_quality]
            score += {'Highly Significant': 3, 'Significant': 2, 'Marginal': 1, 'Not Significant': 0}[sig_bucket]
            score += {'High': 3, 'Medium': 2, 'Low': 1, 'Very Low': 0}[conf_bucket]
            score += {'Optimal': 2, 'Reasonable': 1, 'Extreme': 0}[predictor_bucket]

            if score >= 9:
                overall = 'Excellent'
            elif score >= 6:
                overall = 'Good'
            elif score >= 3:
                overall = 'Fair'
            else:
                overall = 'Poor'
            return overall, {
                'rating_gap': rating_gap,
                'gap_quality': gap_quality,
                'confidence_bucket': conf_bucket,
                'significance_bucket': sig_bucket,
                'predictor_bucket': predictor_bucket,
                'composite_score': score
            }

        # Binding capture quality classification
        def classify_capture(row):
            f1 = sn(row['F1'])
            precision = sn(row['precision'])
            recall = sn(row['recall'])
            tp = sn(row['TP'])
            fp = sn(row['FP'])
            fn = sn(row['FN'])
            total_events = tp + fp + fn

            # Bucket logic based on dynamic thresholds
            prec_bucket = 'Low'
            if precision >= thresholds['precision']['high']:
                prec_bucket = 'High'
            elif precision >= thresholds['precision']['medium']:
                prec_bucket = 'Medium'

            rec_bucket = 'Low'
            if recall >= thresholds['recall']['high']:
                rec_bucket = 'High'
            elif recall >= thresholds['recall']['medium']:
                rec_bucket = 'Medium'

            f1_bucket = 'Poor'
            if f1 >= thresholds['f1']['excellent']:
                f1_bucket = 'Excellent'
            elif f1 >= thresholds['f1']['good']:
                f1_bucket = 'Good'
            elif f1 >= thresholds['f1']['fair']:
                f1_bucket = 'Fair'

            volume_bucket = 'Sparse'
            if total_events >= max_obs * 0.5:
                volume_bucket = 'High'
            elif total_events >= max_obs * 0.2:
                volume_bucket = 'Medium'
            elif total_events >= max_obs * 0.05:
                volume_bucket = 'Low'

            score = 0
            score += {'Excellent': 3, 'Good': 2, 'Fair': 1, 'Poor': 0}[f1_bucket]
            score += {'High': 3, 'Medium': 2, 'Low': 1, 'Sparse': 0}[volume_bucket]
            # Reward balanced precision/recall
            if prec_bucket == 'High' and rec_bucket == 'High':
                score += 3
            elif prec_bucket == 'Medium' and rec_bucket == 'Medium':
                score += 2
            else:
                score += {'High': 2, 'Medium': 1, 'Low': 0}[prec_bucket] + {'High': 2, 'Medium': 1, 'Low': 0}[rec_bucket]

            if score >= 10:
                overall = 'Excellent'
            elif score >= 7:
                overall = 'Good'
            elif score >= 4:
                overall = 'Fair'
            else:
                overall = 'Poor'
            return overall, {
                'f1_bucket': f1_bucket,
                'precision_bucket': prec_bucket,
                'recall_bucket': rec_bucket,
                'event_volume_bucket': volume_bucket,
                'total_events': total_events,
                'tp': tp, 'fp': fp, 'fn': fn,
                'composite_score': score
            }

        # Pattern / insight extraction
        def extract_insights(flow_meta, capture_meta, row):
            insights = []
            # High confidence & predictor strength but poor capture
            if flow_meta['confidence_bucket'] in ['High', 'Medium'] and flow_meta['predictor_bucket'] in ['Optimal','Reasonable'] and capture_meta['f1_bucket'] in ['Poor','Fair']:
                insights.append("Strong flow signal but classification under-performing; review thresholding or labeling.")
            # Zero TP but decent confidence
            if capture_meta['tp'] == 0 and flow_meta['confidence_bucket'] in ['High','Medium']:
                insights.append("No true positives despite confidence; possible mismatch between flow triggers and binding labeling.")
            # High recall low precision
            if capture_meta['recall_bucket'] == 'High' and capture_meta['precision_bucket'] == 'Low':
                insights.append("High recall with low precision – many false alarms; consider raising trigger threshold.")
            # High precision low recall
            if capture_meta['precision_bucket'] == 'High' and capture_meta['recall_bucket'] == 'Low':
                insights.append("High precision but missing events – model conservative; consider lowering threshold.")
            # Important/Critical but poor flow prediction
            if row.get('Constraint_Importance') in ['Critical','Important'] and flow_meta['composite_score'] < 6:
                insights.append("Operational risk: high-impact constraint with weak predictive flow signal.")
            # Significant statistical separation but low confidence
            if flow_meta['significance_bucket'] in ['Highly Significant','Significant'] and flow_meta['confidence_bucket'] in ['Low','Very Low']:
                insights.append("Statistically distinct flow behavior but low confidence – potential data volume issue.")
            # Sparse events hamper evaluation
            if capture_meta['event_volume_bucket'] in ['Sparse','Low'] and capture_meta['f1_bucket'] in ['Poor','Fair']:
                insights.append("Limited event volume; performance metrics may be unstable – accumulate more history.")
            return insights

        # Compute enhanced columns
        flow_quality = []
        flow_meta_list = []
        capture_quality = []
        capture_meta_list = []
        insight_list = []

        # Temporary importance placeholder (will recompute later if not present)
        if 'Constraint_Importance' not in analyzed_data.columns:
            analyzed_data['Constraint_Importance'] = 'Unknown'

        for _, r in analyzed_data.iterrows():
            fq, fq_meta = classify_flow_pred(r)
            cq, cq_meta = classify_capture(r)
            # inject importance for insight rule referencing
            r['Constraint_Importance'] = r.get('Constraint_Importance','Unknown')
            insights = extract_insights(fq_meta, cq_meta, r)
            flow_quality.append(fq)
            flow_meta_list.append(fq_meta)
            capture_quality.append(cq)
            capture_meta_list.append(cq_meta)
            insight_list.append("; ".join(insights) if insights else "No special patterns detected.")

        analyzed_data['Flow_Prediction_Quality'] = flow_quality
        analyzed_data['Binding_Capture_Quality'] = capture_quality
        analyzed_data['Key_Insights'] = insight_list

        # Build refined verbal analysis combining segments
        def build_narrative(idx, row, fq_meta, cq_meta):
            parts = []
            # Importance first (will be recalculated later if missing)
            importance = row.get('Constraint_Importance','Unknown')
            if importance == 'Unknown':
                parts.append("Constraint importance not yet classified.")
            else:
                parts.append(f"Importance: {importance} constraint.")
            # Flow signal description
            parts.append(
                f"Flow prediction: {row['Flow_Prediction_Quality']} (gap {fq_meta['rating_gap']:.1%}, {fq_meta['gap_quality']} separation; confidence {fq_meta['confidence_bucket']}, {fq_meta['significance_bucket']} pattern, predictor {fq_meta['predictor_bucket']})."
            )
            # Capture performance
            parts.append(
                f"Binding capture: {row['Binding_Capture_Quality']} (F1 bucket {cq_meta['f1_bucket']}, precision {cq_meta['precision_bucket']}, recall {cq_meta['recall_bucket']}, volume {cq_meta['event_volume_bucket']} with TP={cq_meta['tp']:.0f}, FP={cq_meta['fp']:.0f}, FN={cq_meta['fn']:.0f})."
            )
            # Insights & recommendation
            if row['Key_Insights'] and row['Key_Insights'] != 'No special patterns detected.':
                parts.append(f"Insights: {row['Key_Insights']}")
            # Recommendation heuristic
            if importance in ['Critical','Important'] and row['Flow_Prediction_Quality'] in ['Excellent','Good'] and row['Binding_Capture_Quality'] in ['Excellent','Good']:
                parts.append("Recommendation: Prioritize – strong predictive and capture performance on a high-impact constraint.")
            elif importance in ['Critical','Important'] and (row['Flow_Prediction_Quality'] in ['Poor','Fair'] or row['Binding_Capture_Quality'] in ['Poor','Fair']):
                parts.append("Recommendation: Remediate – high-impact but weak performance; investigate thresholds, data quality, or model calibration.")
            elif importance in ['Low','Moderate'] and row['Flow_Prediction_Quality'] in ['Excellent','Good']:
                parts.append("Recommendation: Monitor opportunistically – good performance on lower-impact constraint.")
            else:
                parts.append("Recommendation: Low priority unless operational context elevates importance.")
            return " ".join(parts)

        narratives = []
        for (i, r), fq_meta, cq_meta in zip(analyzed_data.iterrows(), flow_meta_list, capture_meta_list):
            narratives.append(build_narrative(i, r, fq_meta, cq_meta))
        analyzed_data['Verbal_Analysis'] = narratives

        # Recompute constraint importance more rigorously (reuse earlier logic if not present)
        if 'Constraint_Importance' in analyzed_data.columns and (analyzed_data['Constraint_Importance'] == 'Unknown').any():
            # same logic as earlier importance calculation but robust
            def recompute_importance(row):
                shadow_price_hours = sn(row['Shadow Price num of hours'])
                shadow_price_total = sn(row['Shadow Price total price'])
                tp = sn(row['TP']); fp = sn(row['FP']); fn = sn(row['FN'])
                total_observations = tp + fp + fn
                hours_score = min(shadow_price_hours / max_hours, 1)
                price_score = min(shadow_price_total / max_price, 1)
                obs_score = min(total_observations / max_obs, 1)
                importance_score = hours_score*0.4 + price_score*0.4 + obs_score*0.2
                if importance_score >= 0.7: return 'Critical'
                if importance_score >= 0.4: return 'Important'
                if importance_score >= 0.15: return 'Moderate'
                return 'Low'
            analyzed_data['Constraint_Importance'] = analyzed_data.apply(recompute_importance, axis=1)

        # ------------------------------------------------------------------
        # END enhanced analysis block
        # ------------------------------------------------------------------
        
        # Print summary statistics
        print("\n" + "="*50)
        print("PREDICTION QUALITY ANALYSIS SUMMARY")
        print("="*50)
        
        quality_counts = analyzed_data['Prediction_Quality'].value_counts()
        print("\nOverall Prediction Quality Distribution:")
        for quality, count in quality_counts.items():
            percentage = (count / len(analyzed_data)) * 100
            print(f"  {quality}: {count} ({percentage:.1f}%)")
        
        print(f"\nDetailed Statistics:")
        print(f"  Total Constraints Analyzed: {len(analyzed_data)}")
        print(f"  Excellent/Good Predictions: {quality_counts.get('Excellent', 0) + quality_counts.get('Good', 0)} ({((quality_counts.get('Excellent', 0) + quality_counts.get('Good', 0)) / len(analyzed_data)) * 100:.1f}%)")
        
        # Helper function for safe numeric conversion
        def safe_numeric_display(value, default=0):
            """Safely convert to numeric, handling strings and NaN"""
            try:
                if pd.isna(value):
                    return default
                return float(value)
            except (ValueError, TypeError):
                return default
        
        # Show top performing constraints
        print(f"\nTop 10 Best Performing Constraints:")
        top_constraints = analyzed_data[analyzed_data['Prediction_Quality'].isin(['Excellent', 'Good'])].head(10)
        if len(top_constraints) > 0:
            for idx, row in top_constraints.iterrows():
                f1_val = safe_numeric_display(row['F1'], 0)
                conf_val = safe_numeric_display(row['Confidence'], 0)
                print(f"  {idx[0][:50]}... | Quality: {row['Prediction_Quality']} | F1: {f1_val:.3f} | Confidence: {conf_val:.3f}")
        
        # Save the augmented data with robust existence verification
        today_date = datetime.today().strftime("%Y-%m-%d")
        output_file = f"analyzed_predictions_{CURRENT_ISO}_{today_date}.csv"
        output_path = os.path.join(os.getcwd(), output_file)
        analyzed_data.to_csv(output_path)

        # Verify file creation
        if os.path.exists(output_path):
            print(f"\n✓ Analysis complete! Augmented data saved as: {output_path}")
        else:
            print(f"\n✗ Expected output file not found after save attempt: {output_path}")
            # Attempt a secondary write
            try:
                analyzed_data.to_csv(output_path)
                if os.path.exists(output_path):
                    print(f"✓ File successfully written on second attempt: {output_path}")
                else:
                    print("✗ Second write attempt failed – please check directory permissions or path.")
            except Exception as write_err:
                print(f"✗ Error on second write attempt: {write_err}")

        # List all new analytical columns added for clarity
        added_cols = [
            'Prediction_Quality','F1_Category','Confidence_Level','Precision_Level','Recall_Level','Statistical_Significance','Predictor_Reasonableness',
            'Flow_Prediction_Quality','Binding_Capture_Quality','Key_Insights','Verbal_Analysis','Constraint_Importance'
        ]
        present_added = [c for c in added_cols if c in analyzed_data.columns]
        print(f"✓ Added columns: {', '.join(present_added)}")
        
        return analyzed_data
        
    except Exception as e:
        print(f"✗ Error analyzing merged data: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """Main function to run both scripts and merge results"""
    print("="*60)
    print("RUNNING JACOB'S ANALYSIS PIPELINE")
    print("="*60)
    
    # Get today's date
    today_date = datetime.today().strftime("%Y-%m-%d")
    
    # Check if CSV files already exist
    confusion_matrix_file = f"confusion_matrix_per_constraint_MISO_{today_date}.csv"
    forecast_performance_file = f"Forecast_performace_evaluation_miso_{today_date}.csv"
    
    files_exist = os.path.exists(confusion_matrix_file) and os.path.exists(forecast_performance_file)
    
    if files_exist:
        print(f"✓ Found existing files:")
        print(f"  - {confusion_matrix_file}")
        print(f"  - {forecast_performance_file}")
        print("Skipping script execution and proceeding to merge...")
        results = {"BC_per_constraint2.py": True, "Predictor_confidence_Jacob.py": True}
    else:
        print("CSV files not found. Running scripts to generate them...")
        
        # Scripts to run
        scripts = ["BC_per_constraint2.py", "Predictor_confidence_Jacob.py"]
        
        # Run scripts in parallel
        print("Running scripts in parallel...")
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=2) as executor:
            # Submit both scripts
            futures = {executor.submit(run_script, script): script for script in scripts}
            
            # Collect results
            results = {}
            for future in futures:
                script = futures[future]
                success, output = future.result()
                results[script] = success
                
                if not success:
                    print(f"✗ {script} failed. Output:")
                    print(output)
        
        execution_time = time.time() - start_time
        print(f"\nScripts completed in {execution_time:.2f} seconds")
    
    # Check if both scripts succeeded (or files already existed)
    if all(results.values()):
        print("\n" + "="*60)
        print("MERGING CSV FILES")
        print("="*60)
        
        # Wait a moment for files to be written
        time.sleep(1)
        
        # Merge the CSV files
        merge_data = merge_csv_files()

        if merge_data is not False:
            analysis_result = Analyze_AI(merge_data)
            print("\n" + "="*60)
            if analysis_result is not None:
                print("✓ PIPELINE COMPLETED SUCCESSFULLY!")
            else:
                print("✗ PIPELINE COMPLETED WITH ANALYSIS ERRORS")
            print("="*60)
        else:
            print("\n" + "="*60)
            print("✗ PIPELINE COMPLETED WITH MERGE ERRORS")
            print("="*60)
    else:
        failed_scripts = [script for script, success in results.items() if not success]
        print(f"\n✗ Pipeline failed. Scripts that failed: {failed_scripts}")

if __name__ == "__main__":
    main()