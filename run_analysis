# environment:
# placebo_api_local

import sys
import bisect
sys.path.append('/Users/michael.simantov/Documents/mu-placebo-api')
sys.path.append('/Users/michael.simantov/Documents/mu-placebo-api/placebo_api')
sys.path.append('/Users/michael.simantov/Documents/mu-placebo-api/placebo_api/utils')

# from typing import NamedTuple, Dict
# from placebo_api.utils import api_utils, date_utils
# import api_utils, date_utils
# from placebo.utils import snowflake_utils
# from placebo_api.utils.date_utils import LocalizedDateTime
# from date_utils import LocalizedDateTime
import pandas as pd
import datetime
from tqdm import tqdm
import pickle
# import requests
# import io
import matplotlib.pyplot as plt
from matplotlib.patches import Circle
import numpy as np
import os 
from scipy.interpolate import griddata
import copy
from create_confusion_matirx import plot_confusion_matrix
from scipy.stats import norm

# list of hyperparameters
subplotting = False
to_date = datetime.datetime.now().strftime('%Y-%m-%d')


today_date = datetime.datetime.now().strftime('%Y-%m-%d')
# today_date = '2024-04-20'
today_date = '2024-04-29'
data_to_save = pickle.load(open(f'weekly_report_as_of_{today_date}.pkl', 'rb'))
collected_results_orig = data_to_save['collected_results']
percent_of_ratings = data_to_save['percent_of_ratings']
RT_bindings_NOT_caught_by_MUSE_and_FORECAST_orig = data_to_save['RT_bindings_NOT_caught_by_MUSE_and_FORECAST']

collected_results = {}
RT_bindings_NOT_caught_by_MUSE_and_FORECAST = {}
# keep only the dates that are after 2024-04-12
for date in collected_results_orig:
    # choose the right week:
    # if date > '2024-03-27' and date <= '2024-04-05':
    # if date > '2024-04-05' and date <= '2024-04-12':
    # if date > '2024-04-12' and date <= '2024-04-19':
    if date > '2024-04-19' and date <= '2024-04-26':
        collected_results[date] = collected_results_orig[date]
        
for date in RT_bindings_NOT_caught_by_MUSE_and_FORECAST_orig:
    # choose the right week:
    # if date > '2024-03-27' and date <= '2024-04-05':
    # if date > '2024-04-05' and date <= '2024-04-12':
    # if date > '2024-04-12' and date <= '2024-04-19':
    if date > '2024-04-19' and date <= '2024-04-26':
        RT_bindings_NOT_caught_by_MUSE_and_FORECAST[date] = RT_bindings_NOT_caught_by_MUSE_and_FORECAST_orig[date]



print(' ')
print('_________________________________________________________________________________')
print(f'the following rating percents are available: {percent_of_ratings}')
print(' ')
print(' PROBLEMATIC CONSTRAINTS')
# print the constraints that are not binding for any percent of ratings
collection_of_bad_constraints = {}
unidentified_constraint_counter = 0
unidentified_RT_shadow_price = 0
all_dates = sorted([k for k in RT_bindings_NOT_caught_by_MUSE_and_FORECAST])
for this_date in all_dates:
    problematic_constraints = {}
    for one_case in RT_bindings_NOT_caught_by_MUSE_and_FORECAST[this_date]:
        monitored_uid = one_case[1]
        contingency_uid = one_case[2]
        rating = one_case[4]
        RT_shadow_price = one_case[5]
        this_constraint = tuple((contingency_uid, monitored_uid))
        if not this_constraint in problematic_constraints:
            problematic_constraints[this_constraint] = [(rating, RT_shadow_price)]
        else:
            problematic_constraints[this_constraint].append(rating)

    for this_constraint in problematic_constraints:
        if len(problematic_constraints[this_constraint]) == len(percent_of_ratings):
            # print(f'{this_date}:  {this_constraint}, RT shadow price ${problematic_constraints[this_constraint][0][1]} is problematic for all ratings')
            unidentified_constraint_counter += 1
            unidentified_RT_shadow_price += problematic_constraints[this_constraint][0][1]

            RT_shadow = problematic_constraints[this_constraint][0][1]
            monitored_uid = this_constraint[1]
            if not this_constraint[0] in collection_of_bad_constraints:
                collection_of_bad_constraints[this_constraint[0]] = [(this_date, monitored_uid, RT_shadow)]
            else: 
                collection_of_bad_constraints[this_constraint[0]].append((this_date, monitored_uid, RT_shadow))
        
for this_constraint in collection_of_bad_constraints:
    total_RT = 0
    for this_date, monitored_uid, RT_shadow in collection_of_bad_constraints[this_constraint]:
        total_RT += RT_shadow
    print(f'Contingency: {this_constraint}, total RT shadow price ${total_RT:.2f}')
    for this_date, monitored_uid, RT_shadow in collection_of_bad_constraints[this_constraint]:
        print(f'    {this_date}:  {monitored_uid}, RT shadow price ${RT_shadow:.2f}')
    print(' ')




# The code below aggregates all the results for the entire week.
Counters = []
daily_confusion_FP = []
daily_confusion_FN = []
daily_confusion_TP = []
num_of_points = []
# original_points = []  # Initialize variables to store original points
point_ind = []  # Initialize variables to store the index of the merged point
# original_pnts = []
radius_for_points = [] 
titles = ['index', 'FP_for_chart', 'FN_for_chart', 'TP', 'FP', 'FN', 'Date', 'Monitored_uid', 'Contingency_uid', 'Num Hours', 'Rating', 'orig_rating', 'observed_RT_shadow_price']
weekly_info = pd.DataFrame(columns=titles)

# At this point we have to save. Any graphing will be done based on the saved data to the DataFrame
# options to give the user: 
# 1. Choose a range of dates
# 2. For some dates there are well less than 10 points. Check why
# 3. Choose a specific monitored_uid and contingency_uid
# 4. Choose a specific rating
# 5. Choose to show only data with RT shadow price > Threshold
# 6. Choose to show only data with number of hours less (or more) than a threshold


# i=0
# for date_of_forecast in collected_results:
#     for false_positive, false_negative, TP, FP, FN, tomorrow_date, monitored_uid, contingency_uid, num_points, rating, observed_RT_shadow_price in collected_results[date_of_forecast]:
#         new_row = {'index':i, 'FP_for_chart': false_positive, 'FN_for_chart': false_negative, 'TP': TP, 'FP': FP, 'FN': FN, 'Date': tomorrow_date, 'Monitored_uid': monitored_uid, 'Contingency_uid': contingency_uid, 'Num Hours': num_points, 'Rating': rating, 'observed_RT_shadow_price':observed_RT_shadow_price}
#         weekly_info.loc[len(weekly_info)] = new_row
#         i += 1

identified_constraint_counter = 0
identified_RT_shadow_price = 0
# the following for loop is to choose the best rating for each combination of monitored_uid and contingency_uid:
counter = 1000000
for date_of_forecast in collected_results:
    collected_results_summary = [l for l in collected_results[date_of_forecast]]
    identified_constraint_counter += len(collected_results_summary)
    identified_RT_shadow_price += np.sum([l[12] for l in collected_results_summary])

    #convert collected_results_summary to a DataFrame
    collected_results_summary = pd.DataFrame(collected_results_summary)
    collected_results_summary.columns = ['counter', 'false_positive', 'false_negative', 'TP', 'FP', 'FN', 'tomorrow_date', 'monitored_uid', 'contingency_uid', 'num_points', 'rating', 'orig_rating', 'observed_RT_shadow_price']
    
    # for every combination of monitored_uid and contingency_uid, keep the row with the lowest sum of false_positive + false_negative. Add collected_results_summary['rating']/1000 so that in case we have a tie, the one with the highest rating will be chosen.
    collected_results_summary['sum_FP_FN'] = collected_results_summary['false_positive'] + collected_results_summary['false_negative'] - collected_results_summary['rating']/1000
    collected_results_summary = collected_results_summary.sort_values(by='sum_FP_FN')
    collected_results_summary = collected_results_summary.drop_duplicates(subset=['monitored_uid', 'contingency_uid'], keep='first')
    # collected_results[date_of_forecast] = collected_results_summary[['counter', 'false_positive', 'false_negative', 'TP', 'FP', 'FN', 'tomorrow_date', 'monitored_uid', 'contingency_uid', 'num_points', 'rating', 'orig_rating', 'observed_RT_shadow_price']].values.tolist()
    
    # change the rating at each row to become -1:
    collected_results_summary['rating'] = -1

    #convert the DataFrame back to a dictionary and add this dictionary to collected_results
    for i in collected_results_summary[['counter', 'false_positive', 'false_negative', 'TP', 'FP', 'FN', 'tomorrow_date', 'monitored_uid', 'contingency_uid', 'num_points', 'rating', 'orig_rating', 'observed_RT_shadow_price']].values.tolist():
        i[0] = counter
        counter += 1
        collected_results[date_of_forecast].append(tuple(i))
    
percent_of_ratings.append(-1)
print(' ')
print(f'Percent of unidentified constraints: {(100 * unidentified_constraint_counter / (identified_constraint_counter + unidentified_constraint_counter)):.1f}%')
print(f'Percent of unidentified RT shadow price: {(100 * unidentified_RT_shadow_price / (identified_RT_shadow_price + unidentified_RT_shadow_price)):.1f}%')
print(' ')
i=0
for date_of_forecast in collected_results:
    for counter, false_positive, false_negative, TP, FP, FN, tomorrow_date, monitored_uid, contingency_uid, num_points, rating, orig_rating, observed_RT_shadow_price in collected_results[date_of_forecast]:
        new_row = {'index':counter, 'FP_for_chart': false_positive, 'FN_for_chart': false_negative, 'TP': TP, 'FP': FP, 'FN': FN, 'Date': tomorrow_date, 'Monitored_uid': monitored_uid, 'Contingency_uid': contingency_uid, 'Num Hours': num_points, 'Rating': rating, 'orig_rating': orig_rating, 'observed_RT_shadow_price':observed_RT_shadow_price}
        weekly_info.loc[len(weekly_info)] = new_row
        
# report on false positive and false negative
num_FP = len(weekly_info.query('Rating == -1 and FP_for_chart  == 1 and FN_for_chart == 0'))
num_FN = len(weekly_info.query('Rating == -1 and FP_for_chart  == 0 and FN_for_chart == 1'))
num_total_cases = len(weekly_info.query('Rating == -1'))
num_TP = num_total_cases - num_FP - num_FN
print(f'False Positive Cases: ({num_FP} in total)')
print(weekly_info.query('Rating == -1 and FP_for_chart  == 1 and FN_for_chart == 0'))
print(' ')
print(f'False Negative Cases: ({num_FN} in total)')
print(weekly_info.query('Rating == -1 and FP_for_chart  == 0 and FN_for_chart == 1'))
print(' ')
print(f'Number of True Positive Cases: {num_TP}')

print(' ')
print('Confusion_matrix:')
print(f'FP: {100 * num_FP/num_total_cases:.1f}, FN: {100 * num_FN/num_total_cases:.1f}, TP: {100 * num_TP/num_total_cases:.1f}')
print('     ')

plot_confusion_matrix(num_TP/num_total_cases, num_FP/num_total_cases, num_FN/num_total_cases)

def save_historica_printout_results(df):
    historic_printout_results = df.values.tolist()
    # import os 
    # import pickle
    # Assuming you have the historic_printout_results dictionary in memory 
    # my_date = { "2024-04-20": "Some data", "2024-04-21": "More data"} 
    
    # Load the dictionary from the file 
    # try: 
    #     with open("collected_date_20240420.pkl", "rb") as file: 
    #         my_data_from_file = pickle.load(file) 
    # except FileNotFoundError: 
    #     my_data_from_file = 
    
    # Merge the dictionaries 
    # my_data_from_file.update(historic_printout_results) 
    
    # Save the updated dictionary to a new file 
    new_file_name = "histogram_printout_collection_temp.pkl" 
    with open(new_file_name, "wb") as file: 
        pickle.dump(historic_printout_results, file) 
        file.flush() 
        os.fsync(file.fileno()) 
    # Replace the original file with the new file 
    original_file_name = "histogram_printout_collection.pkl" 
    os.replace(new_file_name, original_file_name) 
    
    print("Data saved successfully.")

    return


def num_of_days_in_the_blob(i):
    return len(weekly_info.query('index in @point_ind[@i]'))


# bins = [0, .5, .9, 1]  # for the histogram: Number of bins as needed
bins = [0, .1, .5, 1]  # for the histogram: Number of bins as needed
def weight_on_num_of_days_in_the_blob(i, percent_of_rating):
    title = 'weight: num of days in the blob'
    total_days = 0
    for j in range(len(daily_confusion_FP)):
        num_of_days = len(weekly_info.query('index in @point_ind[@j]'))
        total_days += num_of_days
    num_of_days = len(weekly_info.query('index in @point_ind[@i]'))
    # calculation for histogram
    hist_num_days, _ = np.histogram([row['TP_for_chart'] for index, row in weekly_info.query('Rating == @percent_of_rating').iterrows()], bins=bins)
    hist_total_days = len(weekly_info.query('Rating == @percent_of_rating'))
    return (100 * num_of_days / total_days), (100 * hist_num_days / hist_total_days), title
def weight_on_number_of_hours_in_the_blob(i, percent_of_rating):
    title = 'weight: number of hours in the blob'
    hist_num_hours, _ = np.histogram([row['TP_for_chart'] for index, row in weekly_info.query('Rating == @percent_of_rating').iterrows()], bins=bins, weights=weekly_info.query('Rating == @percent_of_rating')['Num Hours'])
    hist_total_hours = weekly_info.query('Rating == @percent_of_rating')['Num Hours'].sum()
    return (100*num_of_points[i] / sum(num_of_points)), (100 *  hist_num_hours / hist_total_hours), title
# def weight_on_average_num_of_active_hours_per_day(i):
#     title = 'weight: average num of active hours per day'
#     total = 0
#     for j in range(len(daily_confusion_FP)):
#         num_of_days = len(weekly_info.query('index in @point_ind[@j]'))
#         total += num_of_points[j] / num_of_days
#     num_of_days = len(weekly_info.query('index in @point_ind[@i]'))
#     return (100 * num_of_points[i] / num_of_days / total), title
def weight_on_num_of_active_hours_per_day(i, percent_of_rating):
    title = 'weight: num of active hours per day'
    # total = 0
    # for j in range(len(daily_confusion_FP)):
    #     num_of_days = len(weekly_info.query('index in @point_ind[@j]'))
    #     total += num_of_points[j] / num_of_days
    num_of_days = len(weekly_info.query('index in @point_ind[@i]'))
    return (num_of_points[i] / num_of_days), None, title
def weight_on_total_RT_shadow_price(i, percent_of_rating):
    title = 'weight: total RT shadow price'
    total_shadow_price = 0
    for j in range(len(daily_confusion_FP)):
        RT_shadow_price_of_this_blob = weekly_info.query('index in @point_ind[@j]').observed_RT_shadow_price.sum()
        total_shadow_price += RT_shadow_price_of_this_blob
    # total_shadow_price = weekly_info.observed_RT_shadow_price.sum()
    RT_shadow_price_of_this_blob = weekly_info.query('index in @point_ind[@i]').observed_RT_shadow_price.sum()
    #histogram calculations:
    hist__RT_shadow_price, _ = np.histogram([row['TP_for_chart'] for index, row in weekly_info.query('Rating == @percent_of_rating').iterrows()], bins=bins, weights=weekly_info.query('Rating == @percent_of_rating')['observed_RT_shadow_price'])
    hist_total_RT_shadow_price = hist__RT_shadow_price.sum()
    return (100*RT_shadow_price_of_this_blob / total_shadow_price), (100 * hist__RT_shadow_price / hist_total_RT_shadow_price), title
def weight_on_average_RT_shadow_prices_per_day(i, percent_of_rating):
    title = 'weight: average RT shadow prices per day'
    total = 0
    for j in range(len(daily_confusion_FP)):
        RT_shadow_price_of_this_blob = weekly_info.query('index in @point_ind[@j]').observed_RT_shadow_price.sum()
        num_of_days = len(weekly_info.query('index in @point_ind[@j]'))
        total += RT_shadow_price_of_this_blob / num_of_days
    RT_shadow_price_of_this_blob = weekly_info.query('index in @point_ind[@i]').observed_RT_shadow_price.sum()
    num_of_days = len(weekly_info.query('index in @point_ind[@i]'))
    # histogram calculations:
    hist_total_RT_shadow_price, _ = np.histogram([row['TP_for_chart'] for index, row in weekly_info.query('Rating == @percent_of_rating').iterrows()], bins=bins, weights=weekly_info.query('Rating == @percent_of_rating')['observed_RT_shadow_price'])
    hist_num_days, _ = np.histogram([row['TP_for_chart'] for index, row in weekly_info.query('Rating == @percent_of_rating').iterrows()], bins=bins)
    hist_total_shadow_price = (hist_total_RT_shadow_price / hist_num_days).sum()
    return (100 * RT_shadow_price_of_this_blob / total / num_of_days), (100 * hist_total_RT_shadow_price / hist_num_days / hist_total_shadow_price), title

collection_of_criteria = [weight_on_num_of_days_in_the_blob, weight_on_number_of_hours_in_the_blob, weight_on_num_of_active_hours_per_day, weight_on_total_RT_shadow_price, weight_on_average_RT_shadow_prices_per_day]

# def on_click(event, daily_confusion_FP=daily_confusion_FP, daily_confusion_FN=daily_confusion_FN, num_of_points=num_of_points, radius_for_points=radius_for_points): 
def on_click(event):
    if event.button == 1:         # Left mouse button 
        for i in range(len(daily_confusion_FP)): 
            circle = Circle((np.sqrt(daily_confusion_FP[i]), np.sqrt(daily_confusion_FN[i])), radius=radius_for_points[i]) 
            if circle.contains_point((event.xdata, event.ydata)):
                print(' ')
                num_of_days = len(weekly_info.query('index in @point_ind[@i]'))
                print(f" Number of days: {num_of_days}")
                print(f" Total (and average) number of hours: {num_of_points[i]}, ({num_of_points[i] / num_of_days}:.2f)")
                total_RT_shadow_price = weekly_info.query('index in @point_ind[@i]').observed_RT_shadow_price.sum()
                print(f" Total (and average) RT shadow price: {total_RT_shadow_price} ({total_RT_shadow_price / num_of_days}:.2f)")
                print(f" False Positive: {daily_confusion_FP[i]}") 
                print(f" False Negative: {daily_confusion_FN[i]}")

                print(weekly_info.query('index in @point_ind[@i]'))
                print(' ')
                break



weekly_info['TP_for_chart'] = 1 - (weekly_info['FP_for_chart'] + weekly_info['FN_for_chart'])
# new_row = {'title':title, 'Rating': percent_of_rating, 'bin':this_bin, 'value':this_hist_value, 'date_begining':date_begining, 'date_ending':date_ending}
# printout_collection = {}
# printout_collection['title'] = []
# printout_collection['Rating'] = []
# printout_collection['bin'] = []
# printout_collection['value'] = []
# printout_collection['date_begining'] = []
# printout_collection['date_ending'] = []

# Load the dictionary from the file 
try: 
    with open("histogram_printout_collection.pkl", "rb") as file: 
        past_histogram_printout_collection = pickle.load(file) 
except FileNotFoundError: 
    past_histogram_printout_collection = [] 
    

titles = ['title', 'Rating', 'bin', 'value', 'date_begining', 'date_ending']
histogram_collection_of_data = []
# historic_printout_results = pd.DataFrame(columns=titles)
colors = ['green', 'orange', 'red']

def create_plot(Radius_to_unify_points):

    def calculate_radius(i):
        return np.interp(daily_confusion_TP, (0, 1), (.03, .2))[i]

    len_percent_of_ratings = len(percent_of_ratings)
    if subplotting:
        fig,axs = plt.subplots(len_percent_of_ratings,5, figsize=(30,6*len_percent_of_ratings))
    
    for ind_rating, percent_of_rating in enumerate(percent_of_ratings):
        # weekly_info = weekly_info.query('Rating == @percent_of_rating')
        # weekly_info = weekly_info.query('observed_RT_shadow_price > 0')
        # weekly_info = weekly_info.query('Num Hours > 10')
        # weekly_info = weekly_info.query('Date == @to_date')

        if not subplotting:
            fig,axs = plt.subplots(2,5, figsize=(30,12))

        Counters.clear()
        daily_confusion_FP.clear()
        daily_confusion_FN.clear()
        daily_confusion_TP.clear()
        num_of_points.clear()
        # original_points = []  # Initialize variables to store original points
        point_ind.clear()  # Initialize variables to store the index of the merged point
        # original_pnts = []
        radius_for_points.clear()

        i=0
        for date_of_forecast in collected_results:
            for counter, false_positive, false_negative, TP, FP, FN, tomorrow_date, monitored_uid, contingency_uid, num_points, rating, orig_rating, observed_RT_shadow_price in collected_results[date_of_forecast]:
                if rating != percent_of_rating:
                    continue
                Counters.append(counter)
                daily_confusion_FP.append(false_positive)
                daily_confusion_FN.append(false_negative)
                daily_confusion_TP.append(1 - false_negative - false_positive)
                radius_for_points.append(0)
                num_of_points.append(num_points)
                # original_points.append([(false_positive, false_negative)])  # Initialize original points for each merged point
                point_ind.append([])
        
        daily_confusion_FP_orig = daily_confusion_FP.copy()
        daily_confusion_FN_orig = daily_confusion_FN.copy()
        daily_confusion_TP_orig = daily_confusion_TP.copy()
        num_of_points_orig = num_of_points.copy()

        # daily_confusion_FP    # daily_confusion_FN     # num_of_points    # # original_points    # point_ind    # radius_for_points    #
        Points_unified = []
        # some of the points are very close to each other, so we need to unify them into one big point:
        for i,cnt_i in enumerate(Counters):
            if cnt_i in Points_unified:
                continue
            point_ind[i].append(cnt_i)
            # for j in range(len(daily_confusion_FP)-1,i,-1):
            j = len(Counters)
            for cnt_j in Counters[-1:i:-1]:
                j -= 1
                if cnt_j in Points_unified:
                    continue
                Radius_to_unify_points = calculate_radius(i)
                if abs(daily_confusion_FP[i] - daily_confusion_FP[j]) < Radius_to_unify_points and abs(daily_confusion_FN[i] - daily_confusion_FN[j]) < Radius_to_unify_points:
                    # original_points[i].append((daily_confusion_FP[j], daily_confusion_FN[j])) # Add original point to the corresponding merged point 
                    point_ind[i].append(cnt_j)
                    daily_confusion_FP[i] = (daily_confusion_FP[i]*num_of_points[i] + daily_confusion_FP[j]*num_of_points[j]) / (num_of_points[i] + num_of_points[j])
                    daily_confusion_FN[i] = (daily_confusion_FN[i]*num_of_points[i] + daily_confusion_FN[j]*num_of_points[j]) / (num_of_points[i] + num_of_points[j])
                    daily_confusion_TP[i] = (daily_confusion_TP[i]*num_of_points[i] + daily_confusion_TP[j]*num_of_points[j]) / (num_of_points[i] + num_of_points[j])
                    num_of_points[i] = num_of_points[i] + num_of_points[j]
                    Points_unified.append(cnt_j)
                
        
        # Eliminate points that were unified
        i = len(Counters) - 1
        #for i in range(len(daily_confusion_FP),-1,-1):
        for cnt_i in Counters[-1::-1]:
            if cnt_i in Points_unified:
                daily_confusion_FP.pop(i)
                daily_confusion_FN.pop(i)
                daily_confusion_TP.pop(i)
                num_of_points.pop(i)
                # original_points.pop(i)
                point_ind.pop(i)
                radius_for_points.pop(i)
            i -= 1

        num_days_in_each_blob = []
        for i in range(len(daily_confusion_FP)):
            num_days_in_each_blob.append(num_of_days_in_the_blob(i))

        for ind, chosen_function in enumerate(collection_of_criteria):
    
            # create contours for equal TP
            # ax = plt.gca()
            # if subplotting == True:
            ax = axs[0, ind]
            # else:
            #     ax = plt.gca()

            ax.set_aspect('equal', adjustable='box')  # Set aspect ratio to be equal
            theta = np.linspace(0, 2*np.pi/4, 100)  # Define angles for the circular grid
            # r = np.arange(0, 1.1, .2)  # Define radii for the circular grid
            r = [np.sqrt(r) for r in [.5, .9, 1]]
            for this_ind, radius in enumerate(r):
                x_circle = radius * np.cos(theta)
                y_circle = radius * np.sin(theta)
                ax.plot(x_circle, y_circle, color=colors[this_ind], linestyle='dashed', alpha=0.7, linewidth=3.7)
                ax.text(radius * np.cos(np.pi/4), radius * np.sin(np.pi/4), f'{100-100*radius**2:.0f}%', style='italic', color='red', fontsize=8, ha='center', va='center')

            values = []
            values_for_histogram = []
            # circle each blob using a radius that is proportional to the number of points in the blob
            for i in range(len(daily_confusion_FP)):
                value, value_histogram, title = chosen_function(i, percent_of_rating)
                values.append(value)
                values_for_histogram.append(value_histogram)
                


            
            # The number of points of each blob and the curcle around it are proportional to the number of points in the blob
            text_size = np.interp(values, (min(values), (max(values) + 100)/2), (10, 30))
            radius_range = np.interp(values, (min(values), (max(values) + 100)/2), (.03, .08))

            for i in range(len(daily_confusion_FP)):
                if daily_confusion_TP[i] >= 0.5:
                    ax.text(np.sqrt(daily_confusion_FP[i]), np.sqrt(daily_confusion_FN[i]), f'{values[i]:.0f}', fontsize=text_size[i], ha='center', va='center', color=colors[0])
                elif daily_confusion_TP[i] > 0.1:
                    ax.text(np.sqrt(daily_confusion_FP[i]), np.sqrt(daily_confusion_FN[i]), f'{values[i]:.0f}', fontsize=text_size[i], ha='center', va='center', color=colors[1])
                else:
                    ax.text(np.sqrt(daily_confusion_FP[i]), np.sqrt(daily_confusion_FN[i]), f'{values[i]:.0f}', fontsize=text_size[i], ha='center', va='center', color=colors[2])
                
            for i in range(len(daily_confusion_FP)):
                # add a circle for each point
                radius_for_points[i] = radius_range[i]
                if daily_confusion_TP[i] >= 0.5:
                    circle = Circle((np.sqrt(daily_confusion_FP[i]), np.sqrt(daily_confusion_FN[i])), radius=radius_for_points[i], fill=False, color=colors[0])
                elif daily_confusion_TP[i] > 0.1:
                    circle = Circle((np.sqrt(daily_confusion_FP[i]), np.sqrt(daily_confusion_FN[i])), radius=radius_for_points[i], fill=False, color=colors[1])
                else:
                    circle = Circle((np.sqrt(daily_confusion_FP[i]), np.sqrt(daily_confusion_FN[i])), radius=radius_for_points[i], fill=False, color=colors[2])
                
                ax.add_patch(circle)

            if percent_of_rating == -1:
                plt.connect('button_press_event', on_click)
            
            ax.set_xticks([np.sqrt(.1), np.sqrt(.5), np.sqrt(.9), 1])
            ax.set_yticks([np.sqrt(.1), np.sqrt(.5), np.sqrt(.9), 1])
            ax.set_xticklabels([f"{.1:.2f}", f"{.5:.2f}", f"{.9:.2f}", f"{1:.2f}"])   
            ax.set_yticklabels([f"{.1:.2f}", f"{.5:.2f}", f"{.9:.2f}", f"{1:.2f}"])  

            ax.set_xlabel('False Positive')
            ax.set_ylabel('False Negative')
            if percent_of_rating == -1:
                ax.set_title(f'{title}: \n rating: BEST')
            else:
                ax.set_title(f'{title}: \n rating: {int(100*percent_of_rating)}%')

            # add grid to the image. The grid should be radial, and reflects the fact that the closer to the center, the better the results
            ax.grid(True)

            # Create a histogram based on TP
            if True:
                ax = axs[1, ind]
                # ind, chosen_function in enumerate(collection_of_criteria)
                if chosen_function == weight_on_num_of_active_hours_per_day:
                    
                    hist_total_hours, _ = np.histogram([row['TP_for_chart'] for index, row in weekly_info.query('Rating == @percent_of_rating').iterrows()], bins=bins, weights=weekly_info.query('Rating == @percent_of_rating')['Num Hours'])
                    hist_num_days, _ = np.histogram([row['TP_for_chart'] for index, row in weekly_info.query('Rating == @percent_of_rating').iterrows()], bins=bins)
                    hist = hist_total_hours / hist_num_days
                    
                    # ax.bar(bins[:-1], hist, width=np.diff(bins), align='edge', edgecolor='black', color='blue', alpha=0.7)
                    ax.bar([1-bin for bin in bins][-1::-1][:-1], hist[-1::-1], width=np.abs(np.diff([1-bin for bin in bins][-1::-1])), align='edge', edgecolor='black', color=['green','orange','red'], alpha=0.5)

                else:
                    hist = values_for_histogram[0]

                    # Plot histogram
                    # ax.bar(bins[:-1], hist, width=np.diff(bins), align='edge', edgecolor='black', color='blue', alpha=0.7)
                    container = ax.bar([1-bin for bin in bins][-1::-1][:-1], hist[-1::-1], width=np.abs(np.diff([1-bin for bin in bins][-1::-1])), align='edge', edgecolor='black', 
                    color=['green','orange','red'], alpha=0.5)
                    

                ax.set_xticks([.25, .7, .95])
                ax.set_xticklabels(['TP > 0.5', '0.5-0.9', '< 0.1'], rotation=-45, fontsize=16)
                ax.set_xlabel('True Positive')
                ax.set_ylabel('Percent')
                ax.set_title(f'Histogram of True Positive, rating = {percent_of_rating}')

                # save data for printout
                date_begining = weekly_info.Date.min()
                date_ending = weekly_info.Date.max()

                # do not delete: This organizes the data in a pandas DataFrame
                # for this_bin, this_hist_value in zip(bins, hist):
                #     new_row = {'title':title, 'Rating': percent_of_rating, 'bin':this_bin, 'value':this_hist_value, 'date_begining':date_begining, 'date_ending':date_ending}
                #     historic_printout_results.loc[len(historic_printout_results)] = new_row

                df = pd.DataFrame(past_histogram_printout_collection, columns=titles)
                # remove duplicates in df
                df = df.drop_duplicates(subset=['title', 'Rating', 'bin', 'value', 'date_begining', 'date_ending'])
                
                # find the percentile of each bin and put it on the histogram
                locs_to_write_percentiles = [.25, .7, .95]
                ind = 0
                historic_means = []
                historic_stds = []
                current_histogram_value = []
                for this_bin, this_hist_value in zip([1-bin for bin in bins][-1::-1], hist[-1::-1]):
                    relevant_values = df.query('title == @title and Rating == @percent_of_rating and bin == @this_bin')['value']
                    # convert relevant_values to np.array
                    relevant_values = np.array(sorted(relevant_values))

                    # find the statistics of the relevant_values
                    historic_means.append(np.mean(relevant_values))
                    historic_stds.append(np.std(relevant_values))
                    current_histogram_value.append(this_hist_value)

                    # find the percentile of this_hist_value in the relevant_values by assuming a normal distribution
                    quantile = norm.cdf(this_hist_value, loc=np.mean(relevant_values), scale=np.std(relevant_values))
                    # quantile = bisect.bisect(relevant_values, this_hist_value) / (len(relevant_values) + 1) + 1 / 2 / (len(relevant_values) + 1)

                    # put the quantile on the histogram, in the middle of the bin
                    if quantile < 0.1 or quantile > 0.9:
                        ax.text(locs_to_write_percentiles[ind], this_hist_value, f'{100*quantile:.0f}%', fontsize=24, ha='center', va='bottom', color='red')
                    else:
                        ax.text(locs_to_write_percentiles[ind], this_hist_value, f'{100*quantile:.0f}%', fontsize=12, ha='center', va='bottom', color='black')
                    ind += 1
                    
                    past_histogram_printout_collection.append([title, percent_of_rating, this_bin, this_hist_value, date_begining, date_ending])
                
                # find the percentile of each bin
                # Calculate yerr values. This is the difference between the current histogram value and the mean of the historic values
                yerr = [[],[]]
                for mean, std, hist_value in zip(historic_means, historic_stds, current_histogram_value):
                    yerr[0].append(max(0, hist_value - mean))
                    yerr[1].append(max(0, -hist_value + mean))
                
                # Add error bars
                # Calculate bin midpoints
                bin_midpoints = [(bins[i] + bins[i+1]) / 2 for i in range(len(bins) - 1)]

                # # Add error bars
                ax.errorbar([1 - bin for bin in bin_midpoints][-1::-1], hist[-1::-1], yerr=yerr, fmt='none', capsize=5, color='black')

    #add the latest data to the historic_printout_results
    df = pd.DataFrame(past_histogram_printout_collection, columns=titles)
    # remove duplicates in df
    df = df.drop_duplicates(subset=['title', 'Rating', 'bin', 'value', 'date_begining', 'date_ending'])
    save_historica_printout_results(df)

############ 

            ###############
            # # Given points
            # # X = [.2, .435, .56, .432, .76, .345]
            # # Y = [.4, .23, .4534, .48, .96, .452]
            # # values = [4, 6, 5, 7, 4.3, 2.9]

            # # Define grid size and generate meshgrid
            # grid_x, grid_y = np.mgrid[min(daily_confusion_FP):max(daily_confusion_FP):100j, min(daily_confusion_FN):max(daily_confusion_FN):100j]

            # # Interpolate values on the grid
            # grid_z = griddata((daily_confusion_FP, daily_confusion_FN), values, (grid_x, grid_y), method='cubic')
            

            # # Create heatmap
            # # ax.imshow(grid_z.T, extent=(min(daily_confusion_FP), max(daily_confusion_FP), min(daily_confusion_FN), max(daily_confusion_FN)), origin='lower', cmap='viridis')
            # ax.imshow(grid_z.T, extent=(min(daily_confusion_FP), max(daily_confusion_FP), min(daily_confusion_FN), max(daily_confusion_FN)), origin='lower', cmap='plasma', vmin=-4, vmax=10)
            # # plt.colorbar(label='Value')
            ###############

    # plt.show() 

    # return daily_confusion_FP, daily_confusion_FN, radius_for_points
    return 

create_plot(Radius_to_unify_points = .15)

plt.show()