# environment:
# placebo_api_local

import sys
sys.path.append('/Users/michael.simantov/Documents/mu-placebo-api')
sys.path.append('/Users/michael.simantov/Documents/mu-placebo-api/placebo_api')
sys.path.append('/Users/michael.simantov/Documents/mu-placebo-api/placebo_api/utils')

# from typing import NamedTuple, Dict
# from placebo_api.utils import api_utils, date_utils
# import api_utils, date_utils
# from placebo.utils import snowflake_utils
# from placebo_api.utils.date_utils import LocalizedDateTime
# from date_utils import LocalizedDateTime
import pandas as pd
import datetime
from tqdm import tqdm
import pickle
# import requests
# import io
import matplotlib.pyplot as plt
from matplotlib.patches import Circle
import numpy as np
# import os 
from scipy.interpolate import griddata
import copy
from create_confusion_matirx import plot_confusion_matrix

# list of hyperparameters
subplotting = False
to_date = datetime.datetime.now().strftime('%Y-%m-%d')


today_date = datetime.datetime.now().strftime('%Y-%m-%d')
today_date = '2024-04-20'
data_to_save = pickle.load(open(f'weekly_report_as_of_{today_date}.pkl', 'rb'))
collected_results_orig = data_to_save['collected_results']
percent_of_ratings = data_to_save['percent_of_ratings']
RT_bindings_NOT_caught_by_MUSE_and_FORECAST_orig = data_to_save['RT_bindings_NOT_caught_by_MUSE_and_FORECAST']

collected_results = {}
RT_bindings_NOT_caught_by_MUSE_and_FORECAST = {}
# keep only the dates that are after 2024-04-12
for date in collected_results_orig:
    #get rid of dates that are before 2024-04-12:
    # if date > '2024-03-27' and date <= '2024-04-05':
    # if date > '2024-04-05' and date <= '2024-04-12':
    if date > '2024-04-12' and date <= '2024-04-19':
        collected_results[date] = collected_results_orig[date]
        
for date in RT_bindings_NOT_caught_by_MUSE_and_FORECAST_orig:
    #get rid of dates that are before 2024-04-12:
    # if date > '2024-03-27' and date <= '2024-04-05':
    # if date > '2024-04-05' and date <= '2024-04-12':
    if date > '2024-04-12' and date <= '2024-04-19':
        RT_bindings_NOT_caught_by_MUSE_and_FORECAST[date] = RT_bindings_NOT_caught_by_MUSE_and_FORECAST_orig[date]



print(' ')
print('_________________________________________________________________________________')
print(f'the following rating percents are available: {percent_of_ratings}')
print(' ')
print(' PROBLEMATIC CONSTRAINTS')
# print the constraints that are not binding for any percent of ratings
collection_of_bad_constraints = {}
unidentified_constraint_counter = 0
unidentified_RT_shadow_price = 0
all_dates = sorted([k for k in RT_bindings_NOT_caught_by_MUSE_and_FORECAST])
for this_date in all_dates:
    problematic_constraints = {}
    for one_case in RT_bindings_NOT_caught_by_MUSE_and_FORECAST[this_date]:
        monitored_uid = one_case[1]
        contingency_uid = one_case[2]
        rating = one_case[4]
        RT_shadow_price = one_case[5]
        this_constraint = tuple((contingency_uid, monitored_uid))
        if not this_constraint in problematic_constraints:
            problematic_constraints[this_constraint] = [(rating, RT_shadow_price)]
        else:
            problematic_constraints[this_constraint].append(rating)

    for this_constraint in problematic_constraints:
        if len(problematic_constraints[this_constraint]) == len(percent_of_ratings):
            # print(f'{this_date}:  {this_constraint}, RT shadow price ${problematic_constraints[this_constraint][0][1]} is problematic for all ratings')
            unidentified_constraint_counter += 1
            unidentified_RT_shadow_price += problematic_constraints[this_constraint][0][1]

            RT_shadow = problematic_constraints[this_constraint][0][1]
            monitored_uid = this_constraint[1]
            if not this_constraint[0] in collection_of_bad_constraints:
                collection_of_bad_constraints[this_constraint[0]] = [(this_date, monitored_uid, RT_shadow)]
            else: 
                collection_of_bad_constraints[this_constraint[0]].append((this_date, monitored_uid, RT_shadow))
        
for this_constraint in collection_of_bad_constraints:
    print(f'Contingency: {this_constraint}')
    for this_date, monitored_uid, RT_shadow in collection_of_bad_constraints[this_constraint]:
        print(f'    {this_date}:  {monitored_uid}, RT shadow price ${RT_shadow}')
    print(' ')




# The code below aggregates all the results for the entire week.
Counters = []
daily_confusion_FP = []
daily_confusion_FN = []
daily_confusion_TP = []
num_of_points = []
# original_points = []  # Initialize variables to store original points
point_ind = []  # Initialize variables to store the index of the merged point
# original_pnts = []
radius_for_points = [] 
titles = ['index', 'FP_for_chart', 'FN_for_chart', 'TP', 'FP', 'FN', 'Date', 'Monitored_uid', 'Contingency_uid', 'Num Hours', 'Rating', 'orig_rating', 'observed_RT_shadow_price']
weekly_info = pd.DataFrame(columns=titles)

# At this point we have to save. Any graphing will be done based on the saved data to the DataFrame
# options to give the user: 
# 1. Choose a range of dates
# 2. For some dates there are well less than 10 points. Check why
# 3. Choose a specific monitored_uid and contingency_uid
# 4. Choose a specific rating
# 5. Choose to show only data with RT shadow price > Threshold
# 6. Choose to show only data with number of hours less (or more) than a threshold


# i=0
# for date_of_forecast in collected_results:
#     for false_positive, false_negative, TP, FP, FN, tomorrow_date, monitored_uid, contingency_uid, num_points, rating, observed_RT_shadow_price in collected_results[date_of_forecast]:
#         new_row = {'index':i, 'FP_for_chart': false_positive, 'FN_for_chart': false_negative, 'TP': TP, 'FP': FP, 'FN': FN, 'Date': tomorrow_date, 'Monitored_uid': monitored_uid, 'Contingency_uid': contingency_uid, 'Num Hours': num_points, 'Rating': rating, 'observed_RT_shadow_price':observed_RT_shadow_price}
#         weekly_info.loc[len(weekly_info)] = new_row
#         i += 1

identified_constraint_counter = 0
identified_RT_shadow_price = 0
# the following for loop is to choose the best rating for each combination of monitored_uid and contingency_uid:
counter = 1000000
for date_of_forecast in collected_results:
    collected_results_summary = [l for l in collected_results[date_of_forecast]]
    identified_constraint_counter += len(collected_results_summary)
    identified_RT_shadow_price += np.sum([l[12] for l in collected_results_summary])

    #convert collected_results_summary to a DataFrame
    collected_results_summary = pd.DataFrame(collected_results_summary)
    collected_results_summary.columns = ['counter', 'false_positive', 'false_negative', 'TP', 'FP', 'FN', 'tomorrow_date', 'monitored_uid', 'contingency_uid', 'num_points', 'rating', 'orig_rating', 'observed_RT_shadow_price']
    
    # for every combination of monitored_uid and contingency_uid, keep the row with the lowest sum of false_positive + false_negative. Add collected_results_summary['rating']/1000 so that in case we have a tie, the one with the highest rating will be chosen.
    collected_results_summary['sum_FP_FN'] = collected_results_summary['false_positive'] + collected_results_summary['false_negative'] - collected_results_summary['rating']/1000
    collected_results_summary = collected_results_summary.sort_values(by='sum_FP_FN')
    collected_results_summary = collected_results_summary.drop_duplicates(subset=['monitored_uid', 'contingency_uid'], keep='first')
    # collected_results[date_of_forecast] = collected_results_summary[['counter', 'false_positive', 'false_negative', 'TP', 'FP', 'FN', 'tomorrow_date', 'monitored_uid', 'contingency_uid', 'num_points', 'rating', 'orig_rating', 'observed_RT_shadow_price']].values.tolist()
    
    # change the rating at each row to become -1:
    collected_results_summary['rating'] = -1

    #convert the DataFrame back to a dictionary and add this dictionary to collected_results
    for i in collected_results_summary[['counter', 'false_positive', 'false_negative', 'TP', 'FP', 'FN', 'tomorrow_date', 'monitored_uid', 'contingency_uid', 'num_points', 'rating', 'orig_rating', 'observed_RT_shadow_price']].values.tolist():
        i[0] = counter
        counter += 1
        collected_results[date_of_forecast].append(tuple(i))
    
percent_of_ratings.append(-1)
print(' ')
print(f'Percent of unidentified constraints: {(100 * unidentified_constraint_counter / (identified_constraint_counter + unidentified_constraint_counter)):.1f}%')
print(f'Percent of unidentified RT shadow price: {(100 * unidentified_RT_shadow_price / (identified_RT_shadow_price + unidentified_RT_shadow_price)):.1f}%')
print(' ')
i=0
for date_of_forecast in collected_results:
    for counter, false_positive, false_negative, TP, FP, FN, tomorrow_date, monitored_uid, contingency_uid, num_points, rating, orig_rating, observed_RT_shadow_price in collected_results[date_of_forecast]:
        new_row = {'index':counter, 'FP_for_chart': false_positive, 'FN_for_chart': false_negative, 'TP': TP, 'FP': FP, 'FN': FN, 'Date': tomorrow_date, 'Monitored_uid': monitored_uid, 'Contingency_uid': contingency_uid, 'Num Hours': num_points, 'Rating': rating, 'orig_rating': orig_rating, 'observed_RT_shadow_price':observed_RT_shadow_price}
        weekly_info.loc[len(weekly_info)] = new_row
        
# report on false positive and false negative
num_FP = len(weekly_info.query('Rating == -1 and FP_for_chart  == 1 and FN_for_chart == 0'))
num_FN = len(weekly_info.query('Rating == -1 and FP_for_chart  == 0 and FN_for_chart == 1'))
num_total_cases = len(weekly_info.query('Rating == -1'))
num_TP = num_total_cases - num_FP - num_FN
print(f'False Positive Cases: ({num_FP} in total)')
print(weekly_info.query('Rating == -1 and FP_for_chart  == 1 and FN_for_chart == 0'))
print(' ')
print(f'False Negative Cases: ({num_FN} in total)')
print(weekly_info.query('Rating == -1 and FP_for_chart  == 0 and FN_for_chart == 1'))
print(' ')
print(f'Number of True Positive Cases: {num_TP}')

print(' ')
print('Confusion_matrix:')
print(f'FP: {100 * num_FP/num_total_cases:.1f}, FN: {100 * num_FN/num_total_cases:.1f}, TP: {100 * num_TP/num_total_cases:.1f}')
print('     ')

plot_confusion_matrix(num_TP/num_total_cases, num_FP/num_total_cases, num_FN/num_total_cases)

def weight_on_num_of_days_in_the_blob(i):
    title = 'weight: num of days in the blob'
    total_days = 0
    for j in range(len(daily_confusion_FP)):
        num_of_days = len(weekly_info.query('index in @point_ind[@j]'))
        total_days += num_of_days
    num_of_days = len(weekly_info.query('index in @point_ind[@i]'))
    # total_days = len(weekly_info)
    return(100 * num_of_days / total_days), title
def weight_on_number_of_hours_in_the_blob(i):
    title = 'weight: number of hours in the blob'
    return (100*num_of_points[i] / sum(num_of_points)), title
def weight_on_average_num_of_active_hours_per_day(i):
    title = 'weight: average num of active hours per day'
    total = 0
    for j in range(len(daily_confusion_FP)):
        num_of_days = len(weekly_info.query('index in @point_ind[@j]'))
        total += num_of_points[j] / num_of_days
    num_of_days = len(weekly_info.query('index in @point_ind[@i]'))
    return (100 * num_of_points[i] / num_of_days / total), title
def weight_on_total_RT_shadow_price(i):
    title = 'weight: total RT shadow price'
    total_shadow_price = 0
    for j in range(len(daily_confusion_FP)):
        RT_shadow_price_of_this_blob = weekly_info.query('index in @point_ind[@j]').observed_RT_shadow_price.sum()
        total_shadow_price += RT_shadow_price_of_this_blob
    # total_shadow_price = weekly_info.observed_RT_shadow_price.sum()
    RT_shadow_price_of_this_blob = weekly_info.query('index in @point_ind[@i]').observed_RT_shadow_price.sum()
    return (100*RT_shadow_price_of_this_blob / total_shadow_price), title
def weight_on_average_RT_shadow_prices_per_day(i):
    title = 'weight: average RT shadow prices per day'
    total = 0
    for j in range(len(daily_confusion_FP)):
        RT_shadow_price_of_this_blob = weekly_info.query('index in @point_ind[@j]').observed_RT_shadow_price.sum()
        num_of_days = len(weekly_info.query('index in @point_ind[@j]'))
        total += RT_shadow_price_of_this_blob / num_of_days
    RT_shadow_price_of_this_blob = weekly_info.query('index in @point_ind[@i]').observed_RT_shadow_price.sum()
    num_of_days = len(weekly_info.query('index in @point_ind[@i]'))
    return (100 * RT_shadow_price_of_this_blob / total / num_of_days), title

collection_of_criteria = [weight_on_num_of_days_in_the_blob, weight_on_number_of_hours_in_the_blob, weight_on_average_num_of_active_hours_per_day, weight_on_total_RT_shadow_price, weight_on_average_RT_shadow_prices_per_day]

# def on_click(event, daily_confusion_FP=daily_confusion_FP, daily_confusion_FN=daily_confusion_FN, num_of_points=num_of_points, radius_for_points=radius_for_points): 
def on_click(event):
    if event.button == 1:         # Left mouse button 
        for i in range(len(daily_confusion_FP)): 
            circle = Circle((np.sqrt(daily_confusion_FP[i]), np.sqrt(daily_confusion_FN[i])), radius=radius_for_points[i]) 
            if circle.contains_point((event.xdata, event.ydata)):
                print(' ')
                num_of_days = len(weekly_info.query('index in @point_ind[@i]'))
                print(f" Number of days: {num_of_days}")
                print(f" Total (and average) number of hours: {num_of_points[i]}, ({num_of_points[i] / num_of_days})")
                total_RT_shadow_price = weekly_info.query('index in @point_ind[@i]').observed_RT_shadow_price.sum()
                print(f" Total (and average) RT shadow price: {total_RT_shadow_price} ({total_RT_shadow_price / num_of_days})")
                print(f" False Positive: {daily_confusion_FP[i]}") 
                print(f" False Negative: {daily_confusion_FN[i]}")

                print(weekly_info.query('index in @point_ind[@i]'))
                print(' ')
                break




def create_plot(Radius_to_unify_points):

    def calculate_radius(i):
        return np.interp(daily_confusion_TP, (0, 1), (.03, .2))[i]

    len_percent_of_ratings = len(percent_of_ratings)
    if subplotting:
        fig,axs = plt.subplots(len_percent_of_ratings,5, figsize=(30,6*len_percent_of_ratings))
    
    for ind_rating, percent_of_rating in enumerate(percent_of_ratings):
        # weekly_info = weekly_info.query('Rating == @percent_of_rating')
        # weekly_info = weekly_info.query('observed_RT_shadow_price > 0')
        # weekly_info = weekly_info.query('Num Hours > 10')
        # weekly_info = weekly_info.query('Date == @to_date')

        if not subplotting:
            fig,axs = plt.subplots(2,5, figsize=(30,12))

        Counters.clear()
        daily_confusion_FP.clear()
        daily_confusion_FN.clear()
        daily_confusion_TP.clear()
        num_of_points.clear()
        # original_points = []  # Initialize variables to store original points
        point_ind.clear()  # Initialize variables to store the index of the merged point
        # original_pnts = []
        radius_for_points.clear()

        i=0
        for date_of_forecast in collected_results:
            for counter, false_positive, false_negative, TP, FP, FN, tomorrow_date, monitored_uid, contingency_uid, num_points, rating, orig_rating, observed_RT_shadow_price in collected_results[date_of_forecast]:
                if rating != percent_of_rating:
                    continue
                Counters.append(counter)
                daily_confusion_FP.append(false_positive)
                daily_confusion_FN.append(false_negative)
                daily_confusion_TP.append(1 - false_negative - false_positive)
                radius_for_points.append(0)
                num_of_points.append(num_points)
                # original_points.append([(false_positive, false_negative)])  # Initialize original points for each merged point
                point_ind.append([])
        
        daily_confusion_FP_orig = daily_confusion_FP.copy()
        daily_confusion_FN_orig = daily_confusion_FN.copy()
        daily_confusion_TP_orig = daily_confusion_TP.copy()
        num_of_points_orig = num_of_points.copy()

        # daily_confusion_FP    # daily_confusion_FN     # num_of_points    # # original_points    # point_ind    # radius_for_points    #
        Points_unified = []
        # some of the points are very close to each other, so we need to unify them into one big point:
        for i,cnt_i in enumerate(Counters):
            if cnt_i in Points_unified:
                continue
            point_ind[i].append(cnt_i)
            # for j in range(len(daily_confusion_FP)-1,i,-1):
            j = len(Counters)
            for cnt_j in Counters[-1:i:-1]:
                j -= 1
                if cnt_j in Points_unified:
                    continue
                Radius_to_unify_points = calculate_radius(i)
                if abs(daily_confusion_FP[i] - daily_confusion_FP[j]) < Radius_to_unify_points and abs(daily_confusion_FN[i] - daily_confusion_FN[j]) < Radius_to_unify_points:
                    # original_points[i].append((daily_confusion_FP[j], daily_confusion_FN[j])) # Add original point to the corresponding merged point 
                    point_ind[i].append(cnt_j)
                    daily_confusion_FP[i] = (daily_confusion_FP[i]*num_of_points[i] + daily_confusion_FP[j]*num_of_points[j]) / (num_of_points[i] + num_of_points[j])
                    daily_confusion_FN[i] = (daily_confusion_FN[i]*num_of_points[i] + daily_confusion_FN[j]*num_of_points[j]) / (num_of_points[i] + num_of_points[j])
                    daily_confusion_TP[i] = (daily_confusion_TP[i]*num_of_points[i] + daily_confusion_TP[j]*num_of_points[j]) / (num_of_points[i] + num_of_points[j])
                    num_of_points[i] = num_of_points[i] + num_of_points[j]
                    Points_unified.append(cnt_j)
                
        
        # Eliminate points that were unified
        i = len(Counters) - 1
        #for i in range(len(daily_confusion_FP),-1,-1):
        for cnt_i in Counters[-1::-1]:
            if cnt_i in Points_unified:
                daily_confusion_FP.pop(i)
                daily_confusion_FN.pop(i)
                daily_confusion_TP.pop(i)
                num_of_points.pop(i)
                # original_points.pop(i)
                point_ind.pop(i)
                radius_for_points.pop(i)
            i -= 1

        for ind, chosen_function in enumerate(collection_of_criteria):
    
            # create contours for equal TP
            # ax = plt.gca()
            # if subplotting == True:
            ax = axs[0, ind]
            # else:
            #     ax = plt.gca()

            ax.set_aspect('equal', adjustable='box')  # Set aspect ratio to be equal
            theta = np.linspace(0, 2*np.pi/4, 100)  # Define angles for the circular grid
            # r = np.arange(0, 1.1, .2)  # Define radii for the circular grid
            r = [np.sqrt(r) for r in [.1, .5, .8, 1]]
            for radius in r:
                x_circle = radius * np.cos(theta)
                y_circle = radius * np.sin(theta)
                ax.plot(x_circle, y_circle, color='red', linestyle='dashed', alpha=0.7, linewidth=0.7)
                ax.text(radius * np.cos(np.pi/4), radius * np.sin(np.pi/4), f'{100-100*radius**2:.0f}%', style='italic', color='red', fontsize=8, ha='center', va='center')

            values = []
            # circle each blob using a radius that is proportional to the number of points in the blob
            for i in range(len(daily_confusion_FP)):
                value, title = chosen_function(i)
                values.append(value)
            
            # The number of points of each blob and the curcle around it are proportional to the number of points in the blob
            text_size = np.interp(values, (min(values), (max(values) + 100)/2), (10, 30))
            radius_range = np.interp(values, (min(values), (max(values) + 100)/2), (.03, .08))

            for i in range(len(daily_confusion_FP)):
                ax.text(np.sqrt(daily_confusion_FP[i]), np.sqrt(daily_confusion_FN[i]), f'{values[i]:.0f}', fontsize=text_size[i], ha='center', va='center')
                
            for i in range(len(daily_confusion_FP)):
                # add a circle for each point
                radius_for_points[i] = radius_range[i]
                circle = Circle((np.sqrt(daily_confusion_FP[i]), np.sqrt(daily_confusion_FN[i])), radius=radius_for_points[i], fill=False, color='black')
                ax.add_patch(circle)

            if percent_of_rating == -1:
                plt.connect('button_press_event', on_click)
            
            # ax.set_xticks([.2, .4, .6, .8, 1])
            # ax.set_yticks([.2, .4, .6, .8, 1])
            # ax.set_xticklabels([f"{.2**2:.2f}", f"{.4**2:.2f}", f"{.6**2:.2f}", f"{.8**2:.2f}", f"{1:.2f}"])   
            # ax.set_yticklabels([f"{.2**2:.2f}", f"{.4**2:.2f}", f"{.6**2:.2f}", f"{.8**2:.2f}", f"{1:.2f}"]) 

            ax.set_xticks([np.sqrt(.1), np.sqrt(.5), np.sqrt(.8), 1])
            ax.set_yticks([np.sqrt(.1), np.sqrt(.5), np.sqrt(.8), 1])
            ax.set_xticklabels([f"{.1:.2f}", f"{.5:.2f}", f"{.8:.2f}", f"{1:.2f}"])   
            ax.set_yticklabels([f"{.1:.2f}", f"{.5:.2f}", f"{.8:.2f}", f"{1:.2f}"])  

            ax.set_xlabel('False Positive')
            ax.set_ylabel('False Negative')
            ax.set_title(f'{title}: \n rating: {int(100*percent_of_rating)}%, {to_date}')

            # add grid to the image. The grid should be radial, and reflects the fact that the closer to the center, the better the results
            ax.grid(True)

############
            if True:
                ax = axs[1, ind]

                # Create a histogram of daily_confusion_TP
                bins = np.linspace(0, 1, 4)  # Adjust the number of bins as needed
                hist, _ = np.histogram([1-TP for TP in daily_confusion_TP], bins=bins, weights=values)

                # Plot histogram
                ax.bar(bins[:-1], hist, width=np.diff(bins), align='edge')
                ax.set_xlabel('True Positive')
                ax.set_ylabel('Percent')
                ax.set_title(f'Histogram of True Positive, rating = {percent_of_rating}')
############ 

            ###############
            # # Given points
            # # X = [.2, .435, .56, .432, .76, .345]
            # # Y = [.4, .23, .4534, .48, .96, .452]
            # # values = [4, 6, 5, 7, 4.3, 2.9]

            # # Define grid size and generate meshgrid
            # grid_x, grid_y = np.mgrid[min(daily_confusion_FP):max(daily_confusion_FP):100j, min(daily_confusion_FN):max(daily_confusion_FN):100j]

            # # Interpolate values on the grid
            # grid_z = griddata((daily_confusion_FP, daily_confusion_FN), values, (grid_x, grid_y), method='cubic')
            

            # # Create heatmap
            # # ax.imshow(grid_z.T, extent=(min(daily_confusion_FP), max(daily_confusion_FP), min(daily_confusion_FN), max(daily_confusion_FN)), origin='lower', cmap='viridis')
            # ax.imshow(grid_z.T, extent=(min(daily_confusion_FP), max(daily_confusion_FP), min(daily_confusion_FN), max(daily_confusion_FN)), origin='lower', cmap='plasma', vmin=-4, vmax=10)
            # # plt.colorbar(label='Value')
            ###############

    # plt.show()

    # return daily_confusion_FP, daily_confusion_FN, radius_for_points
    return

create_plot(Radius_to_unify_points = .15)

plt.show()