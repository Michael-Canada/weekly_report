import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from typing import NamedTuple
import pytz
import io
import os
plt.rcParams["figure.figsize"] = (10, 7)
import requests
import duckdb

import numpy as np


def _get_auth(env_var: str = "SELF"):
    return tuple(os.environ[env_var].split(":"))

AUTH = _get_auth()
def _get_dfm(url, auth=AUTH):
# def _get_dfm(url):
    resp = requests.get(url, auth=auth)
    
    if resp.status_code != 200:
        print(resp.text)
        resp.raise_for_status()
        
    dfm = pd.read_csv(
        io.StringIO(
            resp.text
        )
    )
    
    return dfm


# Run this with the proper task name
# TASK_UID=31ceeb4f-c01c-4e76-a717-291425624c23; for report in load bus generator branch; do mkdir -p $TASK_UID/$report; gsutil -m cp "gs://marginalunit-placebo/runs/20240311t1950z/canonical-topology/$report/$TASK_UID*" $TASK_UID/$report; done


!ls /Users/michael.simantov/Documents/draft/run-export


!ls /Users/michael.simantov/Documents/draft/run-export/31ceeb4f-c01c-4e76-a717-291425624c23


root = "/Users/michael.simantov/Documents/draft/run-export"
task_uid = "31ceeb4f-c01c-4e76-a717-291425624c23"
path = f"{root}/{task_uid}"


bus_t =  duckdb.sql(
    f"""
    SELECT *, string_split(filename, '/')[-1] AS scenario_hour
    FROM READ_PARQUET("{path}/bus/*.parquet", filename=true)
    """
)

# branch_t =  duckdb.sql(
#     f"""
#     SELECT *, string_split(filename, '/')[-1] AS scenario_hour
#     FROM READ_PARQUET("{path}/branch/*.parquet", filename=true)
#     """
# )


generator_t =  duckdb.sql(
    f"""
    SELECT *, string_split(filename, '/')[-1] AS scenario_hour
    FROM READ_PARQUET("{path}/generator/*.parquet", filename=true)
    """
)

load_t =  duckdb.sql(
    f"""
    SELECT *, string_split(filename, '/')[-1] AS scenario_hour
    FROM READ_PARQUET("{path}/load/*.parquet", filename=true)
    """
)


db = duckdb.sql(
    """
    SELECT bus_t.scenario_hour, uid, bus_t.name, pl, pg,  FROM bus_t
    LEFT JOIN (
        SELECT bus_id, scenario_hour, SUM(pl) as pl FROM load_t GROUP BY bus_id, scenario_hour
    ) ld
    ON bus_t.id = ld.bus_id AND bus_t.scenario_hour = ld.scenario_hour
    LEFT JOIN (
        SELECT bus_id, scenario_hour, SUM(pg) as pg FROM generator_t GROUP BY bus_id, scenario_hour
    ) g
    ON bus_t.id = g.bus_id AND bus_t.scenario_hour = g.scenario_hour
    """
)


dfm = db.to_df()
dfm["timestamp"] = pd.to_datetime(
    dfm.scenario_hour.map(lambda v: v.split("--")[-1].split(".")[0])
).dt.tz_convert("US/Central")


dfm


CASE_CODE = "ercot_rt_se_20231023_H17"
COLLECTION = "ercot-rt-se.dev"

df_branches = _get_dfm(f"https://api1.marginalunit.com/reflow/{COLLECTION}/{CASE_CODE}/branches")  # fetch branches
df_mapping_logical = _get_dfm(f"https://api1.marginalunit.com/rms/ercot/grouped-constraints/{COLLECTION}/monitored-branch-mappings")
df_contingencies = _get_dfm(f"https://api1.marginalunit.com/reflow/{COLLECTION}/{CASE_CODE}/contingencies")


# MONITORED_UID = "6036__A,TKWSW,345.0,MGSES,345.0"
# CONTINGENCY_UID = "SMDOPHR5"
CONTINGENCY_UID = "SOZNFRI9"

# monitored = df_mapping_logical[df_mapping_logical.monitored_uid == MONITORED_UID].iloc[0].branch_uid
# monitored = "138_ALV_NAL_1"
monitored = "BIGLAK_PHBL_T1_1"

contingency_branches = list(df_contingencies[df_contingencies.contingency_name == CONTINGENCY_UID].branch_name)
contingency_branches


dfb_reflow = _get_dfm(f"https://api1.marginalunit.com/reflow/{COLLECTION}/{CASE_CODE}/buses?columns=memo,name,pg,pl")
df_sf_reflow = _get_dfm(
    f"https://api1.marginalunit.com/reflow/{COLLECTION}/{CASE_CODE}/constraint_exposure/buses?monitored={monitored}&outaged={','.join(contingency_branches)}"
)


# Check it works:
# f"https://api1.marginalunit.com/reflow/{COLLECTION}/{CASE_CODE}/constraint_exposure/buses?monitored={monitored}&outaged={','.join(contingency_branches)}"


dfm.timestamp.agg(['min', 'max'])


# SELECTED_DATETIME = "2023-10-23 16:00:00-05:00"
SELECTED_DATETIME = "2024-02-27 08:00:00-06:00"  # works!


# make sure data exist
dfm[dfm.timestamp == SELECTED_DATETIME]


dff = pd.merge(
    dfm[dfm.timestamp == SELECTED_DATETIME].set_index("uid", verify_integrity=True)[["name", "pg", "pl"]],
    dfb_reflow.set_index("memo")[["pg", "pl"]],
    left_index=True,
    right_index=True,
    suffixes=("_p", "_r")
).fillna(0)

dff["net_inj_p"] = dff.pg_p - dff.pl_p
dff["net_inj_r"] = dff.pg_r - dff.pl_r

dff = pd.merge(
    dff,
    df_sf_reflow.set_index("bus_memo")[["exposure"]],
    left_index=True,
    right_index=True
)

dff["impact_p"] = dff.net_inj_p * dff.exposure
dff["impact_r"] = dff.net_inj_r * dff.exposure
dff["impact_diff"] = dff.impact_p - dff.impact_r


agg_fn =  {
    c: "sum"
    for c in dff.columns
    if c != "name"
}
agg_fn["exposure"] = "mean"


dffa = dff.groupby("name").agg(agg_fn)
dffa["abs_impact_diff"] = abs(dffa.impact_p - dffa.impact_r)

dffa.plot(
    kind="scatter",
    x="impact_r",
    y="impact_p",
    grid=True
)

x = np.linspace(
    dffa.impact_r.min(),
    dffa.impact_r.max()
)

plt.plot(x, x, color="red")

plt.title(f"Impact diff between Reflow and P&RF for {SELECTED_DATETIME}")


dffa.impact_diff.sum()


-dffa.impact_r.sum()


-dffa.impact_p.sum()


dffa.sort_values("impact_diff", ascending=False).head(10)


dfm[dfm.name == "AMOCOOIL"].groupby("timestamp").sum()[["pl", "pg"]].plot(grid=True)



